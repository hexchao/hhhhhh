ã€ŠA Survey of Federated Learning for Edge Computing: Research Problems and Solutionsã€‹
è¾¹ç¼˜è®¡ç®—åº”ç”¨åœºæ™¯çš„ä¸€ä¸ªsurveyã€‚ï¼ˆæœ‰æåˆ°æ”»å‡»å’Œå‹ç¼©ï¼‰

ã€ŠA Survey on Fault-tolerance in Distributed Optimization and Machine Learningã€‹
å¯¹æœ‰ä¸­å¿ƒå’Œæ— ä¸­å¿ƒçš„æ¶æ„èŠ‚ç‚¹æ•°å®¹å¿æƒ…å†µåšäº†ä¸€ä¸ªsurveyï¼Œæ¯”è¾ƒä¸»æµçš„ä¸€äº›å¯¹æ¯”ç®—æ³•éƒ½åœ¨é‡Œé¢äº†

ã€ŠA Little Is Enough: Circumventing Defenses For Distributed Learningã€‹
å³ä½¿æ˜¯iidæ•°æ®ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¹Ÿèƒ½åœ¨ä¸ä¼šè¢«æ£€æµ‹å‡ºå¼‚å¸¸çš„èŒƒå›´å†…ï¼Œé’ˆå¯¹ä¸åŒçš„èšåˆæ–¹å¼åˆ¶é€ æ‰°åŠ¨æ”»å‡»ï¼ˆå¹¶ä¸”æ˜¯non-omniscientæ”»å‡»ï¼‰å’ŒBackdoor Attackï¼Œé™ä½æ¨¡å‹å‡†ç¡®æ€§æˆ–è€…æ”¶æ•›æ€§ã€‚
å› ä¸ºå‡è®¾äº†æ­£æ€å’Œç‹¬ç«‹åŒåˆ†å¸ƒï¼Œæ‰€ä»¥ä¸éœ€è¦çŸ¥é“å…¶å®ƒèŠ‚ç‚¹çš„æ•°æ®ï¼ŒçŸ¥é“æœ¬èŠ‚ç‚¹çš„æ•°æ®å°±è¶³å¤Ÿæ¨æ–­å‡å€¼å’Œæ ‡å‡†å·®äº†ã€‚

ã€ŠAbnormal Client Behavior Detection in Federated Learningã€‹ï¼ˆå¾®ä¼—é“¶è¡Œï¼‰
åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œå½“ç”¨æˆ·ä¸Šä¼ çš„ä¸­å¿ƒæœåŠ¡å™¨çš„æƒé‡å—åˆ°æ”»å‡»æ—¶ï¼Œdetection-basedçš„æ–¹æ³•æ¯”ä¼ ç»Ÿçš„defense-basedæ–¹æ³•æ›´åŠ æœ‰æ•ˆï¼ˆæœ¬æ–‡ä½¿ç”¨çš„æ˜¯Autoencoder-Basedå¼‚å¸¸æ£€æµ‹ï¼‰ã€‚ä½¿ç”¨credit scoreæ¥ä»£æ›¿åŸæƒé‡è®¡ç®—ä¸­çš„å„èŠ‚ç‚¹æ•°æ®ç‚¹æ¯”ä¾‹ï¼Œcredit scoreç”±å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„anomaly scoreç»™å‡ºã€‚

ã€ŠAdversary-resilient Inference and Machine Learning: From Distributed to Decentralizedã€‹
å…³äºä¸­å¿ƒåŒ–å’Œå»ä¸­å¿ƒåŒ–ç»“æ„çš„Byzantineæ”»å‡»å’ŒæŠµæŠ—åšäº†ä¸€ä¸ªç»¼è¿°ï¼Œå¯å‚è€ƒçš„æ–‡ç« è¾ƒå¤š

ã€ŠAdversary-Resilient Distributed and Decentralized Statistical Inference and Machine Learningã€‹
æœ‰ä¸­å¿ƒå’Œæ— ä¸­å¿ƒåˆ†å¸ƒå¼çš„ç»¼è¿°ï¼Œåˆ—ä¸¾äº†å¸¸è§çš„æŠµæŠ—æ”»å‡»çš„æ–¹æ³•ã€‚

ã€ŠAKSEL: Fast Byzantine SGDã€‹
æå‡ºäº†median-basedçš„èšåˆæ–¹æ³•ï¼ŒAkselã€‚è¿™ç§æ–¹æ³•æ—¶é—´å¤æ‚åº¦è¾ƒä½ï¼Œå¯¹æ¶æ„èŠ‚ç‚¹çš„å®¹å¿ç¨‹åº¦æ˜¯n>2fï¼ŒåŒæ—¶ï¼Œèšåˆåçš„æ¢¯åº¦ä¸çœŸå®æ¢¯åº¦ä¹‹é—´çš„Angular errorä¹Ÿæ›´å°ã€‚

ã€ŠApproximate Byzantine Fault-Tolerance in Distributed Optimizationã€‹
ç›¸å¯¹äºexact fault-toleranceçš„æ¦‚å¿µè€Œè¨€ï¼Œæå‡ºäº†æ›´åŠ å®½æ¾çš„approximate fault-toleranceåœ¨å¾—åˆ°è¿‘ä¼¼æœ€ä¼˜è§£ã€‚ï¼ˆ(f, Îµ)-resilienceï¼‰

ã€ŠAsynchronous Byzantine Machine Learning (the case of SGD)ã€‹
åˆ†å¸ƒå¼å¼‚æ­¥SGDç®—æ³•å¯¹æŠ—æ¶æ„èŠ‚ç‚¹ï¼ŒKrumçš„åˆä½œè€…å†™çš„ã€‚

ã€ŠAuto-weighted Robust Federated Learning with Corrupted Data Sourcesã€‹
Auto-weighted Robust Federated Learning (ARFL)ï¼šé€šè¿‡æ¯”è¾ƒæ¯ä¸ªèŠ‚ç‚¹çš„ç»éªŒé£é™©ä¸æœ€ä½³çš„pä¸ªèŠ‚ç‚¹çš„å¹³å‡ç»éªŒé£é™©ï¼ˆp-averageï¼‰æ¥åˆ†é…æƒé‡ï¼Œå¯ä»¥é™ä½æŸå¤±æ˜æ˜¾æ›´é«˜çš„èŠ‚ç‚¹çš„æƒé‡ï¼Œä»è€Œé™ä½ä»–ä»¬å¯¹å…¨å±€æ¨¡å‹çš„è´¡çŒ®

ã€ŠBASGD: Buffered Asynchronous SGD for Byzantine Learningã€‹
åˆ›æ–°ç‚¹ï¼šç ”ç©¶äº†å¼‚æ­¥çš„Byzantine Learningé—®é¢˜ã€‚ï¼ˆæ­¤å‰å”¯äºŒä¸¤ç¯‡æ–‡ç« ï¼ŒKardamæ— æ³•æŠµæŠ—æ”»å‡»ï¼ŒZeno++éœ€è¦åœ¨ä¸­å¿ƒé¢„å­˜æ•°æ®ï¼‰
åŸºç¡€çš„ASGDçœ‹ä¸å‡ºæ¥äº®ç‚¹ï¼Œè¿›é˜¶çš„Bufferedæ“ä½œï¼šèŠ‚ç‚¹è¢«åˆ†ç»„ï¼Œæäº¤çš„æ¢¯åº¦å…ˆå­˜åœ¨ä¸€ä¸ªBufferä¸­èšåˆã€‚

ã€ŠByzantine Fault-Tolerant Distributed Machine Learning Using Stochastic Gradient Descent (SGD) and Norm-Based Comparative Gradient Elimination (CGE)ã€‹
æœ‰ä¸­å¿ƒçš„åˆ†å¸ƒå¼SGDï¼Œi.i.d
æå‡ºäº†ä¸€ç§æ–°çš„é²æ£’èšåˆæ–¹æ³•ï¼šcomparative gradient elimination (CGE)ï¼Œä¸­å¿ƒèŠ‚ç‚¹æ”¶åˆ°nä¸ªéšæœºæ¢¯åº¦ï¼Œå»æ‰æ¬§å¼èŒƒæ•°æœ€å¤§çš„fä¸ªéšæœºæ¢¯åº¦ï¼Œç”¨å‰©ä¸‹n-fä¸ªæ¢¯åº¦åšaggregationï¼ˆè¿™é‡Œç”¨çš„æ˜¯å‡å€¼ï¼‰

ã€ŠByzantine Fault-Tolerance in Federated Local SGD under 2f-Redundancyã€‹
11é‡Œé¢æŠŠæ¶æ„èŠ‚ç‚¹æ¯”ä¾‹å’Œæ¡ä»¶æ•°è”ç³»èµ·æ¥çš„å¼å­å€’æ˜¯ç¬¬ä¸€æ¬¡è§
è¿™æ˜¯è¯æ˜é‡Œä¸ºäº†ä¿è¯descentå¼•å…¥çš„ä¸€ä¸ªæ¡ä»¶ã€‚29å¼
ç‚¹è¯„ä¸€ä¸‹ï¼šä¸€ï¼Œç®—æ³•å¾ˆç®€å•ï¼ŒæŠŠå·®è·æœ€å¤§çš„fä¸ªæ¨¡å‹æ‰”æ‰ã€‚ä¸è¿‡å€’æ˜¯ç¬¬ä¸€æ¬¡æœ‰äººåšlocal sgdçš„é²æ£’æ€§ã€‚äºŒï¼Œ2f redundancyæœ¬è´¨ä¸Šæ˜¯åœ¨è¯´æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®æ˜¯ä¸€æ ·çš„ï¼Œè¿™ä¸ªå‡è®¾æœ‰ç‚¹å¼ºã€‚åœ¨ç¡®å®šæƒ…å†µåº•ä¸‹ï¼Œè¿™ä¸ªç®—æ³•å¯ä»¥ç²¾ç¡®æ”¶æ•›ã€‚åœ¨éšæœºæƒ…å†µä¸‹ï¼Œç®—æ³•ä¼šæ”¶æ•›åˆ°è·Ÿæ–¹å·®æˆæ­£æ¯”çš„å°é‚»åŸŸé‡Œã€‚äº‹å®ä¸Šè¿™ä¸ªæ–¹å·®å¯ä»¥ç”¨vræ¶ˆæ‰ã€‚
ä¸ä¸€å®šæœ‰è¶³å¤Ÿçš„æ–°æ„ï¼Œåªæ˜¯èƒœåœ¨ç®€å•

ã€ŠByzantine Fault-Tolerant Parallelized Stochastic Gradient Descent for Linear Regressionã€‹
ä½¿ç”¨comparative gradient clipping (CGC)æ¥æä¾›ç®—æ³•é²æ£’æ€§ã€‚åœ¨æœ‰ä¸­å¿ƒçš„ç½‘ç»œä¸­ï¼Œæ¯æ¬¡æŠŠæœ€å¤§çš„fä¸ªï¼ˆå‡è®¾çŸ¥é“æ¶æ„èŠ‚ç‚¹æ•°fï¼‰æ¢¯åº¦åšclippingæ“ä½œï¼šä¿ç•™åŸæ¢¯åº¦æ–¹å‘ï¼Œä½†æ˜¯scaleræ˜¯ç¬¬f+1ä¸ªæ¢¯åº¦çš„å¤§å°ã€‚å…¶å®ƒæ¢¯åº¦ä¸å˜ï¼Œèšåˆæ–¹æ³•è¿˜æ˜¯å‡å€¼ã€‚

ã€ŠBYZANTINE-RESILIENT DECENTRALIZED LEARNINGã€‹
ä¸€ç¯‡æ— ä¸­å¿ƒçš„æŠµæŠ—Byzantineæ”»å‡»çš„åšå£«è®ºæ–‡ï¼Œä¸»è¦åˆ†æäº†ä¸‰ç§Byzantine-resilientæ–¹æ³•ï¼š
1.æ”¶åˆ°é‚»å±…èŠ‚ç‚¹çš„ä¿¡æ¯åï¼Œå…ˆä½¿ç”¨SVMåšäºŒåˆ†ç±»ç­›é€‰å†è¿›è¡Œè®¡ç®—
2.Empirical Risk Minimization (ERM)é—®é¢˜åœ¨Byzantineç½‘ç»œä¸­æ— æ³•è¢«exactly solvedï¼Œä½†æ˜¯æœ¬æ–‡æå‡ºçš„Byzantine-Resilient Decentralized coordinate dEscent (ByRDiE)èƒ½å¤Ÿåœ¨ç»Ÿè®¡ä¸Šå®ç°probably and approximately correct (PAC)  Learnable.
3.ByRDiEéœ€è¦scalar sorting processï¼Œä¸é€‚ç”¨äºå‘é‡å’ŒçŸ©é˜µæƒ…å†µï¼Œå› æ­¤å¼•å…¥æ¢¯åº¦ä¸‹é™Byzantine-resilient decentralized gradient descent (BRIDGE)

ã€ŠByzantine-Resilient Multi-Agent Optimizationã€‹
å¼•å…¥ (Î²,Î³)â€“admissibilityæ¥è¯„ä»·æ— ä¸­å¿ƒåˆ†å¸ƒå¼ç½‘ç»œçš„Byzantine-resilience

ã€ŠByzantine Resilient Non-Convex SVRG with Distributed Batch Gradient Computationsã€‹
åœ¨æœ‰ByzantineèŠ‚ç‚¹çš„æœ‰ä¸­å¿ƒåˆ†å¸ƒå¼ç½‘ç»œä¸­ï¼ŒåŸºäºSVRGæå‡ºäº†åˆ†å¸ƒå¼çš„SCSGç®—æ³•æ¥è§£å†³éå‡¸é—®é¢˜

ã€ŠByzantine-Resilient Secure Federated Learningã€‹
Byzantine-resilient secure aggregation(BREA)ç‰¹ç‚¹ï¼š
i) robustness of the trained model against up to A Byzantine adversaries, 
ii) tolerance against up to D user dropouts, iii) privacy of each local model, against the server and up to T colluding users, as long asNâ‰¥2A+1+max{m+2,D+2T},where m is the number of selected models for aggregation.

ã€ŠByzantine-Robust Learning on Heterogeneous Datasets via Resamplingã€‹
æœ‰ä¸­å¿ƒçš„åˆ†å¸ƒå¼ï¼Œnon-iid
1.è®¾è®¡äº†æ–°çš„æ”»å‡»æ¥ç»•è¿‡é˜²å¾¡ï¼ˆnormalized gradientå’Œmimicå¯ä»¥åˆ©ç”¨æ•°æ®å¼‚æ„æ€§ç»•è¿‡ä¸­å€¼æ–¹æ³•å’Œç¬¦å·èšåˆæ–¹æ³•ï¼‰
2.å°†é‡é‡‡æ ·ä¸åŸæœ‰é²æ£’ç®—æ³•ç»“åˆæŠµæŠ—æ”»å‡»

ã€ŠByzantine Stochastic Gradient Descentã€‹
æå‡ºäº†ä¸€ç§ç®—æ³•ï¼ˆByzantineSGDï¼‰ï¼Œå¹¶åˆ†æäº†å®ƒçš„è¿è¡Œæ•ˆç‡å’ŒæŠµæŠ—èƒ½åŠ›
æ¶æ„èŠ‚ç‚¹å®¹å¿åº¦<1/2

ã€ŠByzantine-Tolerant Machine Learningã€‹
ï¼ˆæ˜¯ã€ŠMachine Learning with Adversaries Byzantine Tolerant Gradient Descentã€‹çš„å®Œå–„ç‰ˆæœ¬ï¼‰
æå‡ºKrumå¹¶è¯´æ˜çº¿æ€§èšåˆæ–¹å¼å¯¹äºæŠµæŠ—æ”»å‡»æ˜¯æ— æ•ˆçš„
æå‡ºå¹¶åº”ç”¨(Î±, f)-Byzantine resilienceï¼Œè¯æ˜Krumçš„æ”¶æ•›æ€§ï¼ˆèšåˆæ–¹å¼æœ‰åŸºäºèšåˆå’ŒåŸºäºå¤šæ•°ï¼Œkrumå±äºåŸºäºå¤šæ•°ï¼‰
å¦å¤–ï¼Œè¯æ˜äº†æ”¶æ•›çš„è®¡ç®—æ—¶é—´ä¸ºO(n2 Â· (d + log n))ï¼Œnæ˜¯èŠ‚ç‚¹æ•°é‡ï¼Œdæ˜¯ç‰¹å¾ç»´æ•°

ã€ŠDetection and Isolation of Adversaries in Decentralized Optimization for Non-Strongly Convex Objectivesã€‹
é’ˆå¯¹éå¼ºå‡¸ç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ£€æµ‹å’Œéš”ç¦»æ¶æ„èŠ‚ç‚¹çš„å»ä¸­å¿ƒé²æ£’ç®—æ³•ï¼šdecentralized robust subgradient push (RSGP)ï¼ˆåŸºäºPush-Sumï¼‰

ã€ŠDistributed Approximate Newton's Method Robust to Byzantine Attackersã€‹
å¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯æŠµæŠ—ä¸€é˜¶æ”»å‡»ï¼ˆæ¢¯åº¦ï¼‰ï¼Œè¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸¤ä¸ªæŠµæŠ—äºŒé˜¶æ”»å‡»çš„æ–¹æ³•ã€‚
1.median-based approximate Newton's method (MNM)ï¼ˆä¸­å¿ƒèŠ‚ç‚¹åšå‡ ä½•ä¸­å€¼èšåˆï¼‰
2.comparison-based approximate Newton's method (CNM)ï¼ˆä¸­å¿ƒèŠ‚ç‚¹ä¿ç•™ä¸€äº›å¹²å‡€çš„æ•°æ®ï¼Œè®¡ç®—å‚æ•°ä¹‹åç”¨æ¥filter outï¼‰

ã€ŠDistributed Byzantine Tolerant Stochastic Gradient Descent in the Era of Big Dataã€‹
æå‡ºäº†ä¸¤ç§ä¸é™åˆ¶æ¶æ„èŠ‚ç‚¹æ•°é‡ä¸Šé™ï¼ˆ45/50ä¾ç„¶æœ‰æ•ˆï¼‰çš„Byzantine tolerant SGDç®—æ³•ã€‚
å½“æ¶æ„èŠ‚ç‚¹æ•°ä¸Šç•Œpå·²çŸ¥æ—¶ï¼š
è®¡ç®—èŠ‚ç‚¹æ”¶é›†ç¦»è‡ªå·±è·ç¦»æœ€è¿‘çš„N-p-1ä¸ªèŠ‚ç‚¹çš„å‚æ•°ï¼Œå–å‡å€¼åç”¨æ¥æ›´æ–°è‡ªå·±çš„å‚æ•°ï¼ˆæ›´æ–°åéœ€è¦åŠ å½’ä¸€åŒ–æ­¥éª¤ï¼‰
å½“æ¶æ„èŠ‚ç‚¹æ•°ä¸Šç•Œpå·²çŸ¥æ—¶ï¼š
é€‰æ‹©ç¦»è‡ªå·±è¶³å¤Ÿè¿‘ä¸”å‚æ•°ä¸‹é™æ–¹å‘ä¸€è‡´çš„èŠ‚ç‚¹çš„å‚æ•°ï¼Œå–å‡å€¼åæ›´æ–°ã€‚
ï¼ˆä¸€ä¸ªæƒ³æ³•ï¼šæ¯è½®è¿­ä»£èŠ‚ç‚¹é€‰æ‹©æ—¶éœ€è¦æ‰€æœ‰èŠ‚ç‚¹é—´å‚æ•°å…±äº«ï¼Œä¸­å¿ƒèŠ‚ç‚¹é€šè®¯å‹åŠ›è¾ƒå¤§ï¼Ÿï¼‰

ã€ŠDETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregationã€‹
ï¼ˆæœ‰ä¸­å¿ƒï¼‰DETOXç»“åˆäº†ç®—æ³•å†—ä½™robust aggregationå’Œé²æ£’èšåˆalgorithmic redundancyï¼Œåˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š1.filtering stepï¼Œç”¨å†—ä½™æ¥å‰Šå¼±æ¶æ„èŠ‚ç‚¹çš„å½±å“ï¼Œ2.hierarchical aggregation stepï¼Œä¸sotaçš„é²æ£’èšåˆæ–¹æ³•ç»“åˆã€‚å…¶ä¸­ç¬¬äºŒæ­¥ä¹Ÿæœ‰ä¸¤ä¸ªå°æ­¥éª¤ï¼Œé¦–å…ˆå°†è¿‡æ»¤åçš„æ¢¯åº¦åˆ’åˆ†ä¸ºå°‘é‡ç»„è¿›è¡Œèšåˆï¼Œç„¶åå¯¹å¹³å‡æ¢¯åº¦ä½¿ç”¨é²æ£’èšåˆä»¥è¿›ä¸€æ­¥æœ€å°åŒ–æ‹œå åº­æ¢¯åº¦çš„å‰©ä½™ç—•è¿¹å½±å“

ã€ŠDistributed Momentum for Byzantine-resilient Learningã€‹
åœ¨æœ‰ä¸­å¿ƒçš„ç½‘ç»œèŠ‚ç‚¹ä¸­ï¼Œworkerä½¿ç”¨åŠ¨é‡èƒ½å¤Ÿå‡å°‘ä¸»èŠ‚ç‚¹æ”¶åˆ°çš„å¥½èŠ‚ç‚¹çš„variance-norm ratioï¼Œå¢å¼ºèšåˆé²æ£’æ€§ã€‚
 Momentum consists in summing a series of past gradients with the new one using an exponential decay factor Î¼ (0 < Î¼ < 1), instead of using the new gradient alone.

ã€ŠDistributed Newton Can Communicate Less and Resist Byzantine Workersã€‹
æœ‰ä¸­å¿ƒï¼Œé«˜æ•ˆæ²Ÿé€šçš„åˆ†å¸ƒå¼äºŒé˜¶ä¼˜åŒ–ç®—æ³•ï¼ŒCOMRADE (COMunication-efficient and Robust Approximate Distributed nEwton)ã€‚workerå…ˆå‹ç¼©ä¿¡æ¯å†å‘é€ï¼ˆæ¢¯åº¦å’ŒHessiançŸ©é˜µï¼‰ï¼Œæ¯ä¸ªworkeræ¯æ¬¡è¿­ä»£åªå‘ä¸­å¿ƒèŠ‚ç‚¹å‘é€ä¸€æ¬¡ä¿¡æ¯ã€‚
æŠµæŠ—æ–¹å¼ï¼šåŸºäºèŒƒæ•°çš„æ¢¯åº¦è¿‡æ»¤ï¼ˆå¯¹èŒƒæ•°æœ€å°çš„ä¸€éƒ¨åˆ†æ¢¯åº¦å–å‡å€¼ï¼‰

ã€ŠDistributed Optimization Under Adversarial Nodesã€‹
æ— ä¸­å¿ƒçš„åˆ†å¸ƒå¼ä¼˜åŒ–é—®é¢˜
1.è¯æ˜äº†ä¸å­˜åœ¨â€œæ—¢èƒ½åœ¨æ— æ”»å‡»æƒ…å†µä¸‹æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œåˆèƒ½æŠµå¾¡è®¾è®¡å¥½çš„æ”»å‡»â€çš„ç®—æ³•
2.ä»‹ç»äº†Local Filtering (LF) Dynamicsè¿™ç§consensus-basedçš„åˆ†å¸ƒå¼ä¼˜åŒ–åè®®ï¼Œå¹¶ç ”ç©¶äº†F-localå’ŒF-totalåœ¨ä¸åŒæ”»å‡»ä¸‹çš„è¡¨ç°æƒ…å†µ
3.åœ¨ä¸€å®šæ¡ä»¶ä¸‹ï¼Œæœªé­å—æ”»å‡»çš„èŠ‚ç‚¹èƒ½å¤Ÿæ”¶æ•›åˆ°ä»–ä»¬æœ¬åœ°ç›®æ ‡å‡½æ•°çš„æœ€å°å€¼çš„å‡¸åŒ…å†…
4. Maximum F-local setsçš„è§„æ¨¡è¶Šå¤§ï¼Œç®—æ³•å®‰å…¨æ€§è¶Šå·®ï¼›The size of themaximum F-local setsæ˜¯ä¸€ä¸ªNP-hardé—®é¢˜ã€‚

ã€ŠDistributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descentã€‹
è¾ƒæ—©çš„ä¸€ç¯‡ï¼ˆ2017ï¼‰ï¼Œåˆ†æäº†å‡ ä½•ä¸­å€¼åšæŠµæŠ—æ—¶çš„ç®—æ³•å¤æ‚åº¦
è¯æ˜ï¼ˆä¾æ¦‚ç‡ï¼‰ä¸€è‡´æ”¶æ•›ï¼Œå‚æ•°ä»¥ä¸€å®šçš„è¯¯å·®åœ¨O(logN)æ¬¡è¿­ä»£æ”¶æ•›
ä¸€ä¸ªç‰¹ç‚¹ï¼šèŠ‚ç‚¹æå¤šï¼Œåšä¸­å€¼æ“ä½œä¹‹å‰è¿˜éœ€è¦å…ˆmini-batch
æ¶æ„èŠ‚ç‚¹å®¹å¿åº¦ï¼š2(1 + Îµ )q â‰¤ m 

ã€ŠDynamic Federated Learning Model for Identifying Adversarial Clientsã€‹
é€šè¿‡åŠ¨æ€åœ°åˆ¤æ–­æ¶æ„èŠ‚ç‚¹å¹¶æ‘’é™¤æ¶æ„èŠ‚ç‚¹ä¿¡æ¯æé«˜æ¨¡å‹è®­ç»ƒæ•ˆæœå¹¶å‡è½»è®­ç»ƒä»£ä»·ã€‚
åŸç†ï¼šï¼ˆInduced Ordered Weighted Averaging (IOWA) operatorï¼‰æ ¹æ®å¯¹èšåˆè¿‡ç¨‹ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„è´¡çŒ®åšåŠ æƒã€‚

ã€ŠEF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedbackã€‹
ç›®å‰å…³äºError feedback (EF)çš„ç ”ç©¶å¤§å¤šæœ‰ä»¥ä¸‹å±€é™æ€§ï¼š1.åªé€‚ç”¨äºå•ç‚¹ï¼Œ2.å‡è®¾è¿‡å¼ºï¼ˆæ¯”å¦‚å…¨å±€æ¢¯åº¦æœ‰ç•Œæˆ–è€…è¿­ä»£ä¾èµ–æ€§çš„å‡è®¾ï¼‰ï¼Œ3.æ— åå‹ç¼©ç®—å­ï¼ˆå¢åŠ é€šä¿¡æˆæœ¬ï¼‰ã€‚
EF21ï¼ˆ2021å¹´æå‡ºï¼‰èƒ½å¤Ÿåœ¨åŸºç¡€å‡è®¾ä¸‹è¾¾åˆ°ğ‘‚(1/ğ‘‡)çš„æ”¶æ•›ç‡ï¼Œå¹¶ä¸”é€‚ç”¨äºå¼‚æ„æ•°æ®ã€‚ç®—æ³•æ€è·¯è·Ÿgradient trackingæœ‰ç‚¹åƒ

ã€ŠExploiting Heterogeneity in Robust Federated Best-Arm Identificationã€‹
Non-IID+multi-armed bandits
Fed-SELæœºåˆ¶ï¼š
1.æ¯ä¸ªclientåœ¨æœ¬åœ°arm-setæ‰¾åˆ°æœ€å¥½çš„arm
2.ä¸­å¿ƒèŠ‚ç‚¹ä¸æœ¬åœ°èŠ‚ç‚¹é€šä¿¡ï¼Œæ‰¾åˆ°æœ€å¥½çš„arm

ã€ŠFall of Empires Breaking Byzantine-tolerant SGD by Inner Product Manipulationã€‹
è¯æ˜åœ¨inner production manipulationæ”»å‡»ä¸‹coordinate-wise medianå’ŒKrumæ–¹æ³•å¯èƒ½æ˜¯ä¸å¯é çš„ã€‚
â€œåŸºæœ¬æƒ³æ³•æ˜¯å½“æ¥è¿‘æœ€ä¼˜è§£æ—¶ï¼Œæ­£å¸¸èŠ‚ç‚¹çš„å¹³å‡éšæœºæ¢¯åº¦æ¥è¿‘äº0ï¼Œé‚£ä¹ˆå³ä½¿medianå’Œkrumèƒ½å¤Ÿä¿è¯èšåˆç»“æœç¦»æ­£å¸¸èŠ‚ç‚¹çš„å¹³å‡éšæœºæ¢¯åº¦å¾ˆè¿‘ï¼Œæ–¹å‘ä¹Ÿæœ‰å¯èƒ½æ˜¯ç›¸åçš„â€
æœ€åæ–‡ç« æå‡ºéœ€è¦ä¿®æ”¹å…³äºByzantine Toleranceçš„å®šä¹‰ï¼ˆDefinition 4. (DSSGD-Byzantine Tolerance)ï¼‰

ã€ŠFast Machine Learning with Byzantine Workers and Serversã€‹
ï¼ˆâ€åˆ˜å¤‡â€œç®—æ³•ï¼Œä¹‹å‰æœ‰è¿‡å…³ç¾½å’Œå¼ é£äº†ï¼‰
1.å‡è®¾æ‰€æœ‰èŠ‚ç‚¹å’ŒæœåŠ¡å™¨å‡ä¸å¯ä¿¡
2.ä¸non-Byzantine resilient algorithmsç›¸æ¯”ï¼Œä¸äº§ç”Ÿé¢å¤–çš„é€šä¿¡è½®æ¬¡
3.æœ¬æ–‡æœºåˆ¶ï¼šå»ºç«‹åœ¨æ¢¯åº¦èšåˆè§„åˆ™ï¼ˆGARï¼‰ä¸Šæ¥æŠµæŠ—æ”»å‡»ï¼Œé€šè¿‡ä»workerèŠ‚ç‚¹çš„å‰¯æœ¬ä¸­è¿‡æ»¤å‡ºä¿¡æ¯ï¼ˆä»å¤šå°æœºå™¨ä¸Šå¤åˆ¶å‚æ•°æœåŠ¡å™¨ï¼Œä½†ä¸ä¿¡ä»»è¿™äº›æœåŠ¡å™¨ï¼‰

ã€ŠFault-Tolerance in Distributed Optimization: The Case of Redundancyã€‹
æœ‰ä¸­å¿ƒã€æ— ä¸­å¿ƒæƒ…å†µéƒ½æœ‰è€ƒè™‘ã€‚
åªæœ‰å½“å¥½èŠ‚ç‚¹æœ‰minimal redundancyæ€§è´¨æ—¶ï¼Œæ‰èƒ½æœ€å°åŒ–å¥½èŠ‚ç‚¹ä¹‹é—´çš„aggregate cost
[2f-redundancy] For a given set of non-faulty agents H, their non-faulty cost functions are said to satisfy 2f - redundancy if for every subset S âŠ† H of size at least n âˆ’ 2f ,
argmin ô°µ\sum_{iâˆˆS} Q_i(w) = argmin ô°µ ô°µ\sum_{iâˆˆH} Q_i(w).
æŠµæŠ—æ–¹å¼ï¼šcomparative gradient clipping (CGC)

ã€ŠFederated Machine Learning: Concept and Applicationsã€‹
æ¨å¼ºçš„è”é‚¦å­¦ä¹ 

ã€ŠGeneralized Byzantine-tolerant SGDã€‹
åŸºäº(Î±, f)-Byzantine resilienceçš„ç†è®ºï¼Œè¯æ˜äº†å‡ ä½•ä¸­å€¼ç­‰ä¸‰ç§ä¸­å€¼æ–¹æ³•çš„æ”¶æ•›æ€§
å¦å¤–ä¹Ÿæåˆ°äº†è®¡ç®—å¼€é”€çš„å¤æ‚åº¦é—®é¢˜

ã€ŠHoldout SGD: Byzantine Tolerant Federated Learningã€‹
1.ä»æ‰€æœ‰èŠ‚ç‚¹ä¸­ï¼ŒéšæœºæŠ½Pä¸ªç”¨æ¥è®¡ç®—æ¢¯åº¦ï¼ŒéšæœºæŠ½Cä¸ªç”¨æ¥æŠ•ç¥¨
2.Pä¸ªèŠ‚ç‚¹ç®—å‡ºå„è‡ªçš„éšæœºæ¢¯åº¦åå‘ç»™ä¸­å¿ƒèŠ‚ç‚¹ï¼Œä¸­å¿ƒèŠ‚ç‚¹æŠŠæ”¶åˆ°çš„éšæœºæ¢¯åº¦å‘é€ç»™Cä¸ªæŠ•ç¥¨èŠ‚ç‚¹ï¼Œæ¯ä¸ªæŠ•ç¥¨èŠ‚ç‚¹æŠŠæ”¶åˆ°çš„Pä¸ªéšæœºæ¢¯åº¦å¸¦å…¥è‡ªå·±èŠ‚ç‚¹çš„æŸå¤±å‡½æ•°åšè®¡ç®—ï¼Œç„¶åæŠŠâ€œPÂ·(1-f) ä¸ªæŸå¤±å‡½æ•°æœ€å°çš„éšæœºæ¢¯åº¦â€å¯¹åº”çš„èŠ‚ç‚¹ä½œä¸ºè‡ªå·±çš„æŠ•ç¥¨ç»“æœè¿”å›ç»™ä¸­å¿ƒèŠ‚ç‚¹
3.ä¸­å¿ƒèŠ‚ç‚¹æ”¶åˆ°Cä¸ªæŠ•ç¥¨èŠ‚ç‚¹çš„æŠ•ç¥¨ç»“æœï¼ˆå¯¹PæŠ•ç¥¨ï¼‰ï¼Œåœ¨æŠ•ç¥¨ç»“æœä¸­å–äº¤é›†ï¼Œç”¨è¿™äº›éšæœºæ¢¯åº¦çš„å‡å€¼æ›´æ–°
æ— ä¸­å¿ƒã€æœ‰ä¸­å¿ƒæƒ…å†µéƒ½æœ‰è®¨è®ºã€‚æ— ä¸­å¿ƒæ¨¡å¼ä¸‹ç®—æ³•æ›´å¤æ‚ä¸€ç‚¹
ByzantineèŠ‚ç‚¹è¦æ±‚ï¼šæœ‰ä¸­å¿ƒ<1/2ï¼Œæ— ä¸­å¿ƒ<1/3

ã€ŠLearning from History for Byzantine Robust Optimizationã€‹
ç›®å‰å…¶ä»–robust aggregation rulesçš„é—®é¢˜ï¼š
1.ç›®å‰çš„é²æ£’èšåˆæ–¹æ³•å› ä¸ºå¯¹å™ªéŸ³æ•æ„Ÿï¼Œæ‰€ä»¥éƒ½ä¼šdivergeï¼ˆå³ä½¿æ²¡æœ‰ByzantineèŠ‚ç‚¹å¹²æ‰°ï¼‰
2.å³ä½¿ä¸€è½®æ”»å‡»ä¼šè¢«aggregationæ¶ˆé™¤ï¼Œå¤šè½®æ”»å‡»ï¼ˆè·¨æ—¶é—´ï¼‰ä¸‹ä¾ç„¶ä¼šæœ‰divergence
æœ¬æ–‡ç®—æ³•ç‰¹ç‚¹ï¼šgradient clipping+momentum+nonconvex
ï¼ˆåŒ…è£…çš„å¾ˆå¥½ï¼Œä¸è¿‡ç¨å¾®æœ‰ç‚¹overclaimï¼‰

ã€ŠLearning to Detect Malicious Clients for Robust Federated Learningã€‹
æœ¬æ–‡ç”¨æ ‡å‡†SGDæ›´æ–°ï¼Œå‡è®¾æ¶æ„æ”»å‡»æ˜¯åŠ å…¥i.i.d.çš„noiseï¼Œä½¿ç”¨spectral anomaly detectionæ–¹æ³•æ¥æ£€æµ‹å‡ºæ¶æ„èŠ‚ç‚¹ï¼Œæ›´æ–°æ—¶ç§»é™¤ä¸€å®šæ•°é‡çš„æ¶æ„èŠ‚ç‚¹ã€‚
å…·ä½“æ¥è¯´ï¼Œä½œè€…è®¤ä¸ºæ²¡æœ‰å™ªå£°ï¼ˆæ”»å‡»ï¼‰æ—¶ï¼Œæ•°æ®å¯ä»¥åµŒå…¥ä¸€ä¸ªä½ç»´ç©ºé—´ï¼›è€Œæ¶æ„èŠ‚ç‚¹åµŒå…¥çš„è¯¯å·®ä¼šæ˜æ˜¾è¦å¤§ã€‚å› æ­¤è®­ç»ƒä¸€ä¸ªencoder-decoderå¯ä»¥æ‰¾åˆ°æ¶æ„èŠ‚ç‚¹â€”â€”å®ƒä»¬çš„è¯¯å·®ä¼šå¤§ã€‚ç„¶åé€šè¿‡è®¾ç½®è‡ªé€‚åº”çš„é˜ˆå€¼ï¼Œå¯ä»¥ç§»é™¤ä¸€äº›æ¶æ„èŠ‚ç‚¹ã€‚
ç†è®ºåªè€ƒè™‘äº†çº¿æ€§æ¨¡å‹+L2æŸå¤±æ—¶æ¶æ„æ”»å‡»çš„å½±å“ã€‚ä¸»è¦è¿˜æ˜¯ä¸€ç¯‡å®ä½œå‹æ–‡ç« ã€‚

ã€ŠLocal Model Poisoning Attacks to Byzantine-Robust Federated Learningã€‹
1ã€è¿™ç¯‡æ–‡ç« ä¸»è¦çš„æƒ³æ³•æ˜¯è®¾è®¡ç‰¹å®šçš„æ”»å‡»æ–¹å¼ï¼Œä½¿å¾—ç°æœ‰é²æ£’ç®—æ³•å­¦åˆ°çš„æ¨¡å‹å¾€é”™è¯¯çš„æ–¹å‘èµ°ã€‚æ€è·¯ä¸Cong Xie, Sanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant sgd by inner product manipulationè¿™ç¯‡æ–‡ç« å¾ˆæ¥è¿‘ã€‚
2ã€ä½œè€…åŒºåˆ†äº†data poisoning attackä¸local model poisoning attackã€‚ä¸ªäººè§‰å¾—ï¼Œåœ¨ç”¨åŸºäºSGDçš„ç®—æ³•æ—¶ï¼Œè¿™ä¸¤ç§æ”»å‡»å·®è·ä¸å¤§ã€‚
3ã€è¿™ç¯‡æ–‡ç« æœ€å¤§çš„é—®é¢˜æ˜¯å‡è®¾æ•°æ®ä¸æ˜¯i.i.d.çš„ã€‚ä»å›¾ä¸‰å¯ä»¥çœ‹å‡ºæ¥ï¼Œå¤§éƒ¨åˆ†é²æ£’ç®—æ³•ï¼ˆé™¤äº†Krumï¼‰åœ¨i.i.d.æƒ…å†µä¸‹è¡¨ç°éƒ½ä¸é”™ï¼Œä½†non-i.i.d.æƒ…å†µä¸‹æ€§èƒ½å°±ä¸‹é™å¾ˆå¿«ã€‚è¯´æ˜ä½œè€…æ²¡æœ‰è®¤çœŸçš„ç ”ç©¶ç°æœ‰é²æ£’ç®—æ³•çš„åŸºæœ¬å‡è®¾ã€‚
4ã€è¿™ç¯‡æ–‡ç« è¿˜è®¾è®¡äº†ä¸¤ç§é˜²å¾¡æ–¹æ³•ï¼ŒåŸºæœ¬æ€è·¯æ˜¯åœ¨ä¸­å¿ƒèŠ‚ç‚¹å¼•å…¥é¢å¤–çš„æµ‹è¯•æ ·æœ¬å¸®åŠ©è¯†åˆ«å¯èƒ½çš„æ¶æ„æ”»å‡»ã€‚æœ¬è´¨ä¸Šå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ç§åˆ©ç”¨redundant dataæŠµæŠ—æ”»å‡»çš„æ–¹æ³•ï¼Œä½†å¤æ‚åº¦æ¯”è¾ƒé«˜ã€‚

ã€ŠMachine Learning with Adversaries Byzantine Tolerant Gradient Descentã€‹
ï¼ˆã€ŠByzantine-Tolerant Machine Learningã€‹çš„åˆæœŸç‰ˆæœ¬ï¼‰
æå‡ºäº†Krumï¼Œå¹¶ç®€è¿°äº†(Î±ï¼Œf)æ”¶æ•›æ€§ï¼Œæ›´åå‘äºsketchï¼Œé™„æœ‰éƒ¨åˆ†è¯æ˜

ã€ŠMitigating Sybils in Federated Learning Poisoningã€‹
å¥³å·«æ”»å‡»çš„æŠµæŠ—ã€‚åœ¨ä¸Šé¢å¤§éƒ¨åˆ†Byzantine resilienceæ–‡ç« ä¸­ï¼Œæ¶æ„èŠ‚ç‚¹çš„æ•°é‡ä¸è¶…è¿‡ä¸€å®šæ•°é‡å‡ ä¹æ˜¯ä¸€é¡¹å¿…ç„¶è¦æ±‚ï¼Œä½†å®é™…ä¸­ï¼Œæ¶æ„ç”¨æˆ·å¯ä»¥è¯•å›¾åˆ›å»ºå¤šä¸ªå±äºè‡ªå·±çš„èŠ‚ç‚¹ï¼Œè½»è€Œæ˜“ä¸¾åœ°æ”»ç ´ä¸Šé¢majority basedçš„ç®—æ³•ã€‚è¿™ç§æ”»å‡»æ–¹å¼å°±å«å¥³å·«æ”»å‡»ã€‚
ç ”ç©¶å¥³å·«æ”»å‡»çš„æ–‡ç« ä¸å¤šï¼Œè¿™é‡Œè¡¥å……ä¸€ç¯‡ï¼Œæ–‡ç« æå‡ºäº†FoolsGoldç®—æ³•ã€‚ç„¶è€Œè¿™ç¯‡æ–‡ç« æŠµæŠ—å¥³å·«æ”»å‡»çš„ç«‹è¶³ç‚¹æ˜¯åœ¨åŸå¸‚èŠ‚ç‚¹çš„Non-IIDæ€§è´¨å’Œå¥³å·«æ”»å‡»èŠ‚ç‚¹çš„ç›¸ä¼¼æ€§ä¸Šã€‚è¯šå®èŠ‚ç‚¹è¿”å›ä¿¡æ¯ä¸ç›¸ä¼¼ï¼Œå¥³å·«èŠ‚ç‚¹ä¹‹é—´éå¸¸ç›¸ä¼¼ï¼Œå› è€Œå¯ä»¥è®¾æ³•æŠŠä»–ä»¬åˆ†è¾¨å¼€æ¥ã€‚
ç†è®ºä¸Šè¿™ç§æ–¹å¼å’ŒåŸæ¥çš„è¦æ±‚æ¶æ„èŠ‚ç‚¹ä¸èƒ½å¤ªå¤šçš„è¦æ±‚å¹¶æ²¡æœ‰ä»€ä¹ˆä¸åŒã€‚å¯¹åŸå¸‚èŠ‚ç‚¹å……åˆ†Non-IIDçš„è¦æ±‚é™åˆ¶äº†ç®—æ³•å¯ç”¨æ€§ã€‚è¯¥æ–‡ä»…ä¾›å‚è€ƒã€‚

ã€ŠON DISTRIBUTED STOCHASTIC GRADIENT DESCENT FOR NONCONVEX FUNCTIONS IN THE PRESENCE OF BYZANTINESã€‹
æœ‰ä¸­å¿ƒçš„åˆ†å¸ƒå¼SGDï¼ŒÎ± < 1/2ï¼Œå¯ç”¨äºéå‡¸ç›®æ ‡å‡½æ•°
è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æ–¹æ³•æ¥æŠµæŠ—Byzantineæ”»å‡»ï¼š
g_{t+1} = { iâˆˆg_{t} :|A_{i}^(t) âˆ’A_{med}^(t) | â‰¤ 4M^2 T âˆ© ||âˆ‡_{i,t} âˆ’ âˆ‡_med|| â‰¤ 4v }
å…¶ä¸­
A_{i}^(t) = sum_{t} ô°“âŸ¨âˆ‡_{i,t} ,âˆ‡_{0,t} âŸ©
A_{med}^(t) = med{A_{1}^(t),...,A_{w}^(t)}

ã€ŠOn the Design of Communication Efficient Federated Learning over Wireless Networksã€‹
åœ¨SignSGDä¸­ï¼Œæ¨¡å‹è¡¨ç°ä¾èµ–äºä¸¤ä¸ªå› ç´ ï¼š1.æœ‰é™æ—¶é—´å†…çš„ä¿¡æ¯äº¤äº’è½®æ•°ï¼ˆä¿¡æ¯äº¤æ¢é€Ÿåº¦ï¼‰ï¼›2.ä¿¡æ¯äº¤äº’æ—¶çš„ä¸­æ–­æ¦‚ç‡ã€‚
æœ¬æ–‡ç ”ç©¶äº†ä¸¤ä¸ªtradeoffï¼š1.å•è½®ä¿¡æ¯äº¤äº’çš„æ—¶é—´ç»™å®šæ—¶ï¼Œä¸­æ–­æ¦‚ç‡å’Œèƒ½è€—çš„tradeoffï¼›2.èƒ½è€—ç»™å®šæ—¶ï¼Œä¿¡æ¯äº¤äº’è½®æ•°å’Œä¸­æ–­æ¦‚ç‡çš„tradeoffã€‚

ã€ŠResilient Distributed Optimization Algorithms for Resource Allocationã€‹
æå‡ºåŸºäºresilient primal-dualçš„æ–¹æ³•ï¼Œå°†ä¸€ç§æ¢å¤å‡å€¼ç‚¹çš„èšåˆæ–¹æ³•ä¸primal-dual data resource allocationï¼ˆPD-DRAï¼‰æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œä¾ç„¶æ˜¯æœ‰ä¸­å¿ƒçš„ç»“æ„ï¼ŒByzantineèŠ‚ç‚¹<1/2

ã€ŠResilient Multi-Robot Target Pursuitã€‹
æ— ä¸­å¿ƒæƒ…å†µï¼Œå¯ä»¥å¿å—nä¸ªé‚»å±…ä¸­[n/d-1] âˆ’ 1ä¸ªæ¶æ„èŠ‚ç‚¹
æŠµæŠ—æ–¹å¼ï¼šåŸºäºä¸­å¿ƒç‚¹çš„ï¼Œé«˜ç»´ç©ºé—´çš„å¹¿ä¹‰ä¸­å€¼ç®—æ³•ï¼Œå¥½èŠ‚ç‚¹å¯ä»¥ä¿è¯æŠŠä¸­å¿ƒç‚¹è½åœ¨å®ƒçš„å¥½é‚»å±…çš„å‡¸åŒ…å†…ã€‚

ã€ŠRobust Distributed Optimization With Randomly Corrupted Gradientsã€‹
Robust Averaging Normalized Gradient Method (RANGE)ç®—æ³•æ­¥éª¤ï¼š1) Temporal gradient averaging, 2) robust aggregation, and 3) gradient normalization
é€‚ç”¨åœºæ™¯ï¼šnovel Markovian Byzantine agent modelï¼ˆæ”»å‡»èŠ‚ç‚¹æ•°é‡æ˜¯ä¸ç¡®å®šçš„ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½å¯èƒ½æ˜¯æ”»å‡»ç»“ç‚¹ï¼‰

ã€ŠRobust federated learning in a heterogeneous environmentã€‹
åœ¨æ•°æ®æ˜¯éiidçš„æƒ…å†µä¸‹ï¼ˆHeterogeneousï¼‰ï¼Œé€šè¿‡clusterè§£å†³è”é‚¦å­¦ä¹ ä¸­çš„Byzantineæ”»å‡»é—®é¢˜ã€‚
ä»ç»Ÿè®¡æ„ä¹‰ä¸Šè¯æ˜äº†Lloydsç®—æ³•çš„å¯é æ€§ï¼ˆLloydsä¸k-meansæœ‰ä¸€äº›åŒºåˆ«ï¼Œä½†æ–‡ä¸­ä¼¼ä¹æ²¡æœ‰åŒºåˆ†ï¼‰

ã€ŠRobust Training in High Dimensions via Block Coordinate Geometric Median Descentã€‹
ç”¨å‡ ä½•ä¸­å€¼ï¼ˆGMï¼‰æ¥åšé²æ£’èšåˆå¤ªè€—æ—¶ï¼Œè¿™ç¯‡æ–‡ç« ç”¨a judiciously chosen block of coordinates+a memory mechanismæ¥èŠ‚çœå¼€é”€ï¼ŒåŒæ—¶è¾¾æˆæ™®é€šGMåœ¨SGDä¸Šçš„éæ¸è¿›æ”¶æ•›ç‡ï¼ˆ1/2æ¶æ„èŠ‚ç‚¹ï¼Œå…‰æ»‘éå‡¸é—®é¢˜ï¼‰

ã€ŠSecure Byzantine-Robust Machine Learningã€‹
åœ¨æœ‰Byzantineæ”»å‡»çš„åˆ†å¸ƒå¼é—®é¢˜ä¸‹ï¼Œæå‡ºäº† two-server protocol.
secure aggregation rule + two non-colluding honest-but-curious servers

ã€ŠSGD: Decentralized Byzantine Resilienceã€‹
GUANYUï¼šå¼‚æ­¥æ›´æ–°ï¼Œå®¹å¿1/3çš„æ¶æ„èŠ‚ç‚¹
åŸºäº(Î±, f)-Byzantine resilientçš„Gradient Aggregation Rules (GARs)
æåˆ°äº†ä¸€äº›æ”»å‡»æ–¹æ³•ï¼Œå¯ä»¥å‚è€ƒã€‚å¦å¤–ä½œä¸ºå†™ä½œæŒ‡å¯¼ä¹Ÿå¾ˆä¸é”™

ã€ŠSignGuard - Byzantine-robust Federated Learning through Collaborative Malicious Gradient Filteringã€‹
ç®—æ³•æ€è·¯ï¼š1.å¯¹æ¯ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦è®¡ç®—æ¨¡é•¿ä¸­ä½æ•°ï¼Œåªè¦ä¸­ä½æ•°é™„è¿‘çš„èŠ‚ç‚¹ã€‚2.å¯¹æ¢¯åº¦åšelement-wiseçš„indexæŠ½æ ·ï¼Œåªå–signä¿¡æ¯ï¼Œç”¨Mean-Shiftèšç±»æ–¹æ³•é€‰æ‹©èŠ‚ç‚¹é›†åˆã€‚å–1å’Œ2çš„äº¤é›†ï¼Œç®—å‡å€¼åšaggregation
ä¸¤ç‚¹ï¼šNon-IIDï¼Œå¯ä»¥æŠµæŠ—â€œLittle is Enoughâ€æ”»å‡»ï¼Œåˆ—å‡ºäº†ä¸ƒç§æ”»å‡»æ–¹å¼ï¼ˆå¯ä»¥å‚è€ƒï¼‰

ã€ŠStochastic-Sign SGD for Federated Learning with Theoretical Guaranteesã€‹
Sign SGDåœ¨æ¯è½®è¿­ä»£çš„ä¸»ä»èŠ‚ç‚¹ä¿¡æ¯äº¤æ¢ä¸­åªä¼ é€’æ¢¯åº¦ç¬¦å·å’Œaggregationå€¼ï¼Œæ•ˆç‡å¾ˆé«˜ä½†æ˜¯åœ¨å¼‚è´¨çš„åˆ†å¸ƒä¸‹ä¸æ”¶æ•›ã€‚æœ¬æ–‡æå‡ºçš„stochastic-sign SGDå’Œ differentially private sign SGDç®—æ³•å¯ä»¥åœ¨å¼‚è´¨æ•°æ®åˆ†å¸ƒä¸‹ä¹Ÿè¾¾åˆ°æ”¶æ•›çš„æ•ˆæœï¼Œè€Œä¸”æ”¶æ•›ç‡ä¸åŒè´¨æƒ…å†µç›¸åŒã€‚
ï¼ˆä½œè€…é‡‘æ—¥æˆSign SGDç›¸å…³è®ºæ–‡è¿˜æœ‰ä¸¤ç¯‡ï¼šDistributed Byzantine Tolerant Stochastic Gradient Descent in the Era of Big Data å’Œ On the Design of Communication Efficient Federated Learning over Wireless Networksï¼‰

ã€ŠThe Hidden Vulnerability of Distributed Learning in Byzantiumã€‹
æå‡ºä¸€ç§é’ˆå¯¹Krumå’ŒGeometric Medianç­‰åŸºäºèŒƒæ•°çš„é²æ£’èšåˆç®—æ³•çš„æ”»å‡»ä»¥åŠå¯¹åº”çš„é˜²å¾¡ç®—æ³•Bulyanã€‚
Krumå’ŒGeoMedæŠµæŠ—æ”»å‡»èƒ½åŠ›éƒ½å¼ºçƒˆä¾èµ–äºè¯šå®èŠ‚ç‚¹ä¹‹é—´æ¢¯åº¦çš„ä¸€è‡´æ€§ã€‚å½“è¯šå®èŠ‚ç‚¹é—´æ¢¯åº¦ä¸ä¸€è‡´æ—¶ï¼Œè¿™ç§åˆ†æ­§ä¼šç•™ç»™æ”»å‡»è€…ä¸€å®šçš„æ“ä½œç©ºé—´ã€‚æ–‡ç« æ„é€ çš„è¿™ç§æ”»å‡»å°±æ˜¯åˆ©ç”¨è¿™ç§åˆ†æ­§æ©ç›–è¿™ç§æ”»å‡»ï¼Œå¹¶åœ¨æ‰€ç•™ä¸‹çš„æ“ä½œç©ºé—´é‡Œå°è¯•æ§åˆ¶æœ€ç»ˆçš„èšåˆç»“æœï¼Œè¿™ç§æ”»å‡»å¯¹Krumå’ŒGeoMedç ´ååŠ›æå¤§ã€‚
æ–‡ç« æå‡ºBulyanç®—æ³•ï¼Œå¯ä»¥çœ‹ä½œKrumå’ŒGeoMedçš„å †å ç‰ˆæœ¬ã€‚ç®—æ³•å…ˆè®¡ç®—é€šå¸¸çš„Krumå’ŒGeoMedï¼Œé€‰å‡ºæ¢¯åº¦é›†åˆé‡Œæœ€é è¿‘èšåˆç»“æœçš„æ¢¯åº¦ï¼Œæ‹¿å‡ºæ¥å¹¶è®°å½•ï¼Œå¤šæ¬¡é‡å¤ã€‚å¯¹è¿™äº›æ‹¿å‡ºæ¥çš„æ¢¯åº¦è¿›è¡Œç®€å•å¹³å‡ï¼Œå°±æ˜¯èšåˆç®—æ³•Bulyanã€‚

ã€ŠTowards Byzantine-resilient Learning in Decentralized Systemsã€‹
æ— ä¸­å¿ƒæƒ…å†µä¸‹çš„Byzantineé—®é¢˜ã€‚æå‡ºäº†MOZIï¼ˆå¢¨å­ï¼‰èšåˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹Byzantineçš„èŠ‚ç‚¹æ•°ä¸ä½œè¦æ±‚ï¼ˆç”šè‡³æ˜¯å¯å˜çš„ï¼‰ã€‚
åœ¨æœ‰ä¸­å¿ƒçš„æƒ…å†µä¸‹ï¼ŒByzantine-resilientæ–¹æ³•ä¸»è¦åˆ†ä¸ºåŸºäºè·ç¦»ï¼ˆDistance-based solutionsï¼‰å’Œç›®æ ‡å‡½æ•°ï¼ˆPerformance-based Solutionsï¼‰ä¸¤ç§ç±»å‹ã€‚ç°æœ‰çš„å»ä¸­å¿ƒåŒ–Byzantine-resilientæ–¹æ³•å¤§å¤šæ˜¯åŸºäºè·ç¦»åšèšåˆï¼Œå¯¹ç‰¹å®šçš„æ”»å‡»æŠµæŠ—åŠ›è¾ƒå¼±ï¼ˆMhamdi[20],[25],[26]ï¼‰ã€‚
åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œå„èŠ‚ç‚¹ä½¿ç”¨é‚»å±…èŠ‚ç‚¹çš„aggregationå’Œè¯¥èŠ‚ç‚¹çš„ç›®æ ‡å‡½æ•°è´Ÿæ¢¯åº¦æ¥æ›´æ–°ã€‚
æœ¬æ–‡ä½¿ç”¨çš„æ˜¯aggregationç®—æ³•MOZIçš„æ€è·¯æ˜¯ï¼š
1.åŸºäºè·ç¦»åœ¨é‚»å±…èŠ‚ç‚¹ä¸­é€‰å‡ºå¯èƒ½çš„å¥½èŠ‚ç‚¹é›†åˆ
2.åŸºäºç›®æ ‡å‡½æ•°è¡¨ç°å†é€‰ä¸€æ¬¡ï¼ˆä¸éœ€è¦å¦å¤–å‡†å¤‡æµ‹è¯•é›†ï¼Œä¾ç„¶ä½¿ç”¨è®­ç»ƒæ•°æ®ï¼‰
æ³¨ï¼š
æœ‰æ”¶æ•›æ€§åˆ†æï¼Œä½†æ²¡æœ‰çœ‹åˆ°å…³äºByzantineèŠ‚ç‚¹æ•°é‡ä»»æ„æ€§çš„åˆ†æï¼ˆå¦‚æœè¿‡åŠï¼Œç¬¬ä¸€æ­¥å°±å®äº†ï¼‰

ã€ŠZeno: Byzantine-suspicious stochastic gradientdescentã€‹
ç¬¬ä¸€ä¸ªè¯æ˜äº†ByzantineèŠ‚ç‚¹æ•°é‡å¤šçš„æ—¶å€™ä¹Ÿèƒ½æ”¶æ•›ï¼šmajority-based algorithmså¯èƒ½ä¼šå¤±è´¥ï¼Œsuspicion-based algorithmèƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜
ä»é™åˆ¶æ–¹å·®çš„è§’åº¦æ¥è¯æ˜
åŸºäºæ–¹æ³•çš„è¯æ˜ï¼Œæš‚æ—¶ä¸ç”¨çœ‹
æ³¨ï¼š
æœ¬æ–‡æå‡ºçš„ç®—æ³•åŸºäºä¸€ä¸ªå‡è®¾ï¼šbyzantineèŠ‚ç‚¹åœ¨æ”»å‡»æ—¶ä¸çŸ¥é“æ­£å¸¸èŠ‚ç‚¹ä¸Šä¼ çš„æ¢¯åº¦
